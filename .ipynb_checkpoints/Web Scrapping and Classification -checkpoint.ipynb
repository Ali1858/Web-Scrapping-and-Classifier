{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import codecs\n",
    "import os.path\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First We collect list of all movies and Filter it according to our Labels , After That Collect Full Review of Each Movie and create features from That."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the page and create BeautifulSoup Object\n",
    "link='https://wogma.com/movies/basic/'\n",
    "source_page = urllib.request.urlopen(link).read()\n",
    "soup = bs.BeautifulSoup(source_page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From soup Object find the table tag\n",
    "# If there is more than one table tag in Page use find_all('table')\n",
    "table = soup.find('table')\n",
    "#find all row tag in table\n",
    "table_rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\bs4\\element.py:1577: UserWarning: has_key is deprecated. Use has_attr(\"class\") instead.\n",
      "  key))\n"
     ]
    }
   ],
   "source": [
    "#Iterate through each row and find all table data tag \n",
    "\n",
    "\n",
    "movie_data = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = []\n",
    "    for col in td:\n",
    "        #Iterate through all the td tags and if tag has class 'listing_synopsis' \n",
    "        # That Tag have link of the movie  \n",
    "        #We extract link from <a> \n",
    "        if col.has_key('class') and col['class'][0] == 'listing_synopsis':\n",
    "            hrefs = col.div.find('a')\n",
    "            value = hrefs.get('href').strip()\n",
    "        else:\n",
    "            value = re.sub(\"[^a-zA-Z0-9’/_.']\",\" \",col.text).strip()\n",
    "        row.append(value)\n",
    "        # append 'Movie name' , 'Movie Rating' and 'Link of Full Review'\n",
    "    movie_data.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total of 1253 moview data\n",
    "len(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We Wil Filter The data \n",
    "#We consider only those movie which has proper class defined by us and discard all remaining data\n",
    "#convert movie rating to class\n",
    "filter_data = []\n",
    "discard = []\n",
    "for each_movie in movie_data:\n",
    "    rating = each_movie[1]\n",
    "    c = 0\n",
    "    if rating == 'Beg or borrow  but do watch' or rating == 'Add to \\'must watch\\' list'  or rating == 'Owner\\'s Pride' :\n",
    "        c = 1\n",
    "    elif rating == 'The keen should rent  else TV/online' or rating == 'Watch if you have nothing better to do' or rating == 'Watch when on TV/online':\n",
    "        c = 2\n",
    "    elif rating == 'Switch channels if it\\'s on cable':\n",
    "        c = 3\n",
    "        \n",
    "    if c !=0:\n",
    "        filter_data.append([each_movie[0],c,each_movie[2]])\n",
    "    else:\n",
    "        discard.append(rating)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 58)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after filtering\n",
    "len(filter_data),len(discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rukh', 2, '/movie/rukh-review/'],\n",
       " ['Jia Aur Jia', 2, '/movie/jia-aur-jia-review/']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filered Movie Data\n",
    "filter_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download reviews ......................\n",
      "all review downloaded at  C:\\Users\\Ali\\Repo\\Web Scrapping and Classifier\\reviews\n"
     ]
    }
   ],
   "source": [
    "#Download Full Review Of Each Movie and write it in txt file and store it\n",
    "path_review='C:\\\\Users\\\\Ali\\\\Repo\\\\Web Scrapping and Classifier\\\\reviews' #path to store full movie review\n",
    "\n",
    "print('download reviews ......................')\n",
    "for index,each_data in enumerate(filter_data):\n",
    "    #print(each_data[0])\n",
    "    \n",
    "    #Read Page of Full Review\n",
    "    link = 'https://www.wogma.com/'+each_data[2]\n",
    "    review_page = urllib.request.urlopen(link).read()\n",
    "    soup = bs.BeautifulSoup(review_page,'lxml')\n",
    "    \n",
    "    #<div> with class 'wogma-review' contain text for our full review\n",
    "    div = soup.find_all('div',class_='wogma-review')\n",
    "    review_txt = div[0].get_text()\n",
    "    store extrated moview review in this path\n",
    "    complete_path = os.path.join(path_review,str(index)+\".txt\")\n",
    "    \n",
    "    with open(complete_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(review_txt)\n",
    "print('all review downloaded at ',path_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "# We will create corpus from all ~ 1200 review and select top 50-5000 vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of all reviews\n",
    "reviews_files = sorted(glob.glob(\"reviews\\*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#corpus of all our review\n",
    "corpus_raw = u\"\"\n",
    "for file in reviews_files:\n",
    "     with codecs.open(file,\"r\",\"utf-8\") as review_file:\n",
    "            corpus_raw += review_file.read()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4022978"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#token of senteces\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39749"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove all special character from our sentence\n",
    "#and create word token from each sentence\n",
    "vocab_list = []\n",
    "for each_sentences in raw_sentences:\n",
    "    if len(each_sentences)>0:\n",
    "        clean = re.sub(\"[^a-zA-Z0-9’]\",\" \",each_sentences)\n",
    "        words =  clean.split()\n",
    "        if len(words)>0:\n",
    "            for word in words:\n",
    "                if word != 'adsbygoogle':\n",
    "                    vocab_list.append(word)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nltk.FreqDist(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 38381), ('a', 19241), ('of', 18082), ('to', 16815), ('and', 15501), ('is', 15145), ('in', 11289), ('that', 10269), ('it', 8329), ('film', 7183), ('you', 6609), ('for', 5473), ('with', 5193), ('I', 5163), ('are', 5091), ('The', 5088), ('as', 4447), ('s', 3870), ('have', 3481), ('his', 3406), ('on', 3335), ('t', 3313), ('be', 3293), ('one', 3116), ('not', 3072), ('this', 3070), ('at', 2870), ('by', 2571), ('all', 2560), ('an', 2546)]\n"
     ]
    }
   ],
   "source": [
    "#Top most common word from vocab list\n",
    "#This word doesnt contrib in our classification \n",
    "print(w.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30172"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique word\n",
    "len(w.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select top 50 to 5000 words from our vocab list\n",
    "word_features = list(w.keys())[50:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have 4500 features for each movie ,We have mark 1 if this feature exist 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(item):\n",
    "    review_path = ('reviews\\{0}.txt'.format(str(item)))\n",
    "    \n",
    "    #read data\n",
    "    review = u\"\"\n",
    "    with codecs.open(review_path,\"r\",\"utf-8\") as review_file:\n",
    "            review = review_file.read()\n",
    "            \n",
    "    #create sentence token\n",
    "    review_sentence_token = tokenizer.tokenize(review)\n",
    "    v = []\n",
    "    #clean sentence and create word token\n",
    "    for s_token in review_sentence_token:\n",
    "        clean = re.sub(\"[^a-zA-Z0-9’]\",\" \",s_token)\n",
    "        words =  clean.split()\n",
    "        for word in words:\n",
    "            v.append(word)\n",
    "    v = set(v)\n",
    "    f = []\n",
    "    #check if that token exist or not\n",
    "    for feature in word_features:\n",
    "        f.append(int(feature in v))\n",
    "    return f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with RandomForest using scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([create_features(item) for item in range(1195)],dtype=float)\n",
    "Y = np.array([filter_data[item][1] for item in range(1195)])\n",
    "#np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X,Y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RanForestClf = RandomForestClassifier()\n",
    "RanForestClf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.6100278552\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y,RanForestClf.predict(test_x))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
