{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import codecs\n",
    "import os.path\n",
    "import glob\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First We collect list of all movies and Filter it according to our Labels , After That Collect Full Review of Each Movie and create features from That."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the page and create BeautifulSoup Object\n",
    "link='https://wogma.com/movies/basic/'\n",
    "source_page = urllib.request.urlopen(link).read()\n",
    "soup = bs.BeautifulSoup(source_page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From soup Object find the table tag\n",
    "# If there is more than one table tag in Page use find_all('table')\n",
    "table = soup.find('table')\n",
    "#find all row tag in table\n",
    "table_rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\bs4\\element.py:1577: UserWarning: has_key is deprecated. Use has_attr(\"class\") instead.\n",
      "  key))\n"
     ]
    }
   ],
   "source": [
    "#Iterate through each row and find all table data tag \n",
    "\n",
    "\n",
    "movie_data = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = []\n",
    "    for col in td:\n",
    "        #Iterate through all the td tags and if tag has class 'listing_synopsis' \n",
    "        # That Tag have link of the movie  \n",
    "        #We extract link from <a> \n",
    "        if col.has_key('class') and col['class'][0] == 'listing_synopsis':\n",
    "            hrefs = col.div.find('a')\n",
    "            value = hrefs.get('href').strip()\n",
    "        else:\n",
    "            value = re.sub(\"[^a-zA-Z0-9’/_.']\",\" \",col.text).strip()\n",
    "        row.append(value)\n",
    "        # append 'Movie name' , 'Movie Rating' and 'Link of Full Review'\n",
    "    movie_data.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total of 1253 moview data\n",
    "len(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We Wil Filter The data \n",
    "#We consider only those movie which has proper class defined by us and discard all remaining data\n",
    "#convert movie rating to class\n",
    "filter_data = []\n",
    "discard = []\n",
    "for each_movie in movie_data:\n",
    "    rating = each_movie[1]\n",
    "    c = 0\n",
    "    if rating == 'Beg or borrow  but do watch' or rating == 'Add to \\'must watch\\' list'  or rating == 'Owner\\'s Pride' :\n",
    "        c = 1\n",
    "    elif rating == 'The keen should rent  else TV/online' or rating == 'Watch if you have nothing better to do' or rating == 'Watch when on TV/online':\n",
    "        c = 2\n",
    "    elif rating == 'Switch channels if it\\'s on cable':\n",
    "        c = 3\n",
    "        \n",
    "    if c !=0:\n",
    "        filter_data.append([each_movie[0],c,each_movie[2]])\n",
    "    else:\n",
    "        discard.append(rating)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 58)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after filtering\n",
    "len(filter_data),len(discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rukh', 2, '/movie/rukh-review/'],\n",
       " ['Jia Aur Jia', 2, '/movie/jia-aur-jia-review/']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filered Movie Data\n",
    "filter_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.save('x',filter_data)\n",
    "filter_data = np.load('x.npy')\n",
    "len(filter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download reviews ......................\n",
      "all review downloaded at  C:\\Users\\Ali\\Repo\\Web Scrapping and Classifier\\reviews\n"
     ]
    }
   ],
   "source": [
    "#Download Full Review Of Each Movie and write it in txt file and store it\n",
    "path_review='C:\\\\Users\\\\Ali\\\\Repo\\\\Web Scrapping and Classifier\\\\reviews' #path to store full movie review\n",
    "\n",
    "print('download reviews ......................')\n",
    "for index,each_data in enumerate(filter_data):\n",
    "    #print(each_data[0])\n",
    "    \n",
    "    #Read Page of Full Review\n",
    "    link = 'https://www.wogma.com/'+each_data[2]\n",
    "    review_page = urllib.request.urlopen(link).read()\n",
    "    soup = bs.BeautifulSoup(review_page,'lxml')\n",
    "    \n",
    "    #<div> with class 'wogma-review' contain text for our full review\n",
    "    div = soup.find_all('div',class_='wogma-review')\n",
    "    review_txt = div[0].get_text()\n",
    "    store extrated moview review in this path\n",
    "    complete_path = os.path.join(path_review,str(index)+\".txt\")\n",
    "    \n",
    "    with open(complete_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(review_txt)\n",
    "print('all review downloaded at ',path_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "# We will create corpus from all ~ 1200 review and select top 50-5000 vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of all reviews\n",
    "reviews_files = sorted(glob.glob(\"reviews\\*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#corpus of all our review\n",
    "corpus_raw = u\"\"\n",
    "for file in reviews_files:\n",
    "     with codecs.open(file,\"r\",\"utf-8\") as review_file:\n",
    "            corpus_raw += review_file.read()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4022978"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#token of senteces\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39749"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove all special character from our sentence\n",
    "#and create word token from each sentence\n",
    "vocab_list = []\n",
    "for each_sentences in raw_sentences:\n",
    "    if len(each_sentences)>0:\n",
    "        clean = re.sub(\"[^a-zA-Z0-9’]\",\" \",each_sentences)\n",
    "        words =  clean.split()\n",
    "        if len(words)>0:\n",
    "            for word in words:\n",
    "                if word != 'adsbygoogle':\n",
    "                    vocab_list.append(word)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nltk.FreqDist(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 38381), ('a', 19241), ('of', 18082), ('to', 16815), ('and', 15501), ('is', 15145), ('in', 11289), ('that', 10269), ('it', 8329), ('film', 7183), ('you', 6609), ('for', 5473), ('with', 5193), ('I', 5163), ('are', 5091), ('The', 5088), ('as', 4447), ('s', 3870), ('have', 3481), ('his', 3406), ('on', 3335), ('t', 3313), ('be', 3293), ('one', 3116), ('not', 3072), ('this', 3070), ('at', 2870), ('by', 2571), ('all', 2560), ('an', 2546)]\n"
     ]
    }
   ],
   "source": [
    "#Top most common word from vocab list\n",
    "#This word doesnt contrib in our classification \n",
    "print(w.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30172"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique word\n",
    "len(w.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select top 50 to 5000 words from our vocab list\n",
    "word_features = list(w.keys())[50:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have 4500 features for each movie ,We have mark 1 if this feature exist 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(item):\n",
    "    review_path = ('reviews\\{0}.txt'.format(str(item)))\n",
    "    \n",
    "    #read data\n",
    "    review = u\"\"\n",
    "    with codecs.open(review_path,\"r\",\"utf-8\") as review_file:\n",
    "            review = review_file.read()\n",
    "            \n",
    "    #create sentence token\n",
    "    review_sentence_token = tokenizer.tokenize(review)\n",
    "    v = []\n",
    "    #clean sentence and create word token\n",
    "    for s_token in review_sentence_token:\n",
    "        clean = re.sub(\"[^a-zA-Z0-9’]\",\" \",s_token)\n",
    "        words =  clean.split()\n",
    "        for word in words:\n",
    "            v.append(word)\n",
    "    v = set(v)\n",
    "    f = []\n",
    "    #check if that token exist or not\n",
    "    for feature in word_features:\n",
    "        f.append(int(feature in v))\n",
    "    return f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with RandomForest using scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([create_features(item) for item in range(1195)],dtype=float)\n",
    "Y = np.array([filter_data[item][1] for item in range(1195)])\n",
    "#np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X,Y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RanForestClf = RandomForestClassifier()\n",
    "RanForestClf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.6100278552\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y,RanForestClf.predict(test_x))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 . \n",
    "# Creating Features using wordvector  by Taking average of all words vector of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#glove vector training on wikipedia \n",
    "#6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download\n",
    "glove_vect_300d = 'glove.6B/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_300D = pd.read_table(glove_vect_300d, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.046560</td>\n",
       "      <td>0.213180</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.458540</td>\n",
       "      <td>-0.035639</td>\n",
       "      <td>0.236430</td>\n",
       "      <td>-0.288360</td>\n",
       "      <td>0.215210</td>\n",
       "      <td>-0.134860</td>\n",
       "      <td>-1.6413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013064</td>\n",
       "      <td>-0.296860</td>\n",
       "      <td>-0.079913</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.031549</td>\n",
       "      <td>0.285060</td>\n",
       "      <td>-0.087461</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>-0.209890</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.255390</td>\n",
       "      <td>-0.257230</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>0.218170</td>\n",
       "      <td>-0.022702</td>\n",
       "      <td>-0.178540</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.058936</td>\n",
       "      <td>-1.3854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>-0.073794</td>\n",
       "      <td>0.221760</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.566860</td>\n",
       "      <td>0.053307</td>\n",
       "      <td>-0.232900</td>\n",
       "      <td>-0.122260</td>\n",
       "      <td>0.354990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>-0.101230</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>0.136270</td>\n",
       "      <td>-0.107210</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>-1.6785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060148</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>-0.119490</td>\n",
       "      <td>0.234450</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>-0.152420</td>\n",
       "      <td>-0.342240</td>\n",
       "      <td>-0.022394</td>\n",
       "      <td>0.136840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.076947</td>\n",
       "      <td>-0.021211</td>\n",
       "      <td>0.212710</td>\n",
       "      <td>-0.722320</td>\n",
       "      <td>-0.139880</td>\n",
       "      <td>-0.122340</td>\n",
       "      <td>-0.175210</td>\n",
       "      <td>0.121370</td>\n",
       "      <td>-0.070866</td>\n",
       "      <td>-1.5721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366730</td>\n",
       "      <td>-0.386030</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.340360</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.183510</td>\n",
       "      <td>-0.291830</td>\n",
       "      <td>-0.046533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.257560</td>\n",
       "      <td>-0.057132</td>\n",
       "      <td>-0.671900</td>\n",
       "      <td>-0.380820</td>\n",
       "      <td>-0.364210</td>\n",
       "      <td>-0.082155</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>-0.082047</td>\n",
       "      <td>0.460560</td>\n",
       "      <td>-1.8477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>-0.597070</td>\n",
       "      <td>0.317340</td>\n",
       "      <td>-0.252670</td>\n",
       "      <td>0.543840</td>\n",
       "      <td>0.063007</td>\n",
       "      <td>-0.049795</td>\n",
       "      <td>-0.160430</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.070621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7    \\\n",
       "0                                                                           \n",
       "the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
       ",   -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
       ".   -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
       "of  -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
       "to  -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
       "\n",
       "          8         9       10     ...          291       292       293  \\\n",
       "0                                  ...                                    \n",
       "the  0.215210 -0.134860 -1.6413    ...    -0.013064 -0.296860 -0.079913   \n",
       ",    0.107560  0.058936 -1.3854    ...     0.075968 -0.014359 -0.073794   \n",
       ".    0.236970  0.328700 -1.6785    ...     0.060148 -0.156190 -0.119490   \n",
       "of   0.121370 -0.070866 -1.5721    ...    -0.366730 -0.386030  0.302900   \n",
       "to  -0.082047  0.460560 -1.8477    ...    -0.012806 -0.597070  0.317340   \n",
       "\n",
       "          294       295       296       297       298       299       300  \n",
       "0                                                                          \n",
       "the  0.195000  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
       ",    0.221760  0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
       ".    0.234450  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
       "of   0.015747  0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
       "to  -0.252670  0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_300D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(item):\n",
    "    review_path = ('reviews\\{0}.txt'.format(str(item)))\n",
    "    \n",
    "    #read data\n",
    "    review = u\"\"\n",
    "    with codecs.open(review_path,\"r\",\"utf-8\") as review_file:\n",
    "            review = review_file.read()\n",
    "            \n",
    "    #create sentence token\n",
    "    review_sentence_token = tokenizer.tokenize(review)\n",
    "    v = []\n",
    "    #clean sentence and create word token\n",
    "    for s_token in review_sentence_token:\n",
    "        clean = re.sub(\"[^a-zA-Z0-9’]\",\" \",s_token)\n",
    "        words =  clean.split()\n",
    "        for word in words:\n",
    "            v.append(word.lower())\n",
    "    #v = set(v)\n",
    "    f = np.zeros(300)\n",
    "    \n",
    "    found_vocab = []\n",
    "    unfound_word_glove= []\n",
    "    for voc in v:\n",
    "    #check if vocab is available in vector\n",
    "    #if yes value of latent features and sum it.\n",
    "        if voc in glove_300D.index:\n",
    "            f += glove_300D.loc[voc].as_matrix()\n",
    "            found_vocab.append(voc)\n",
    "        else:\n",
    "            unfound_word_glove.append(voc)\n",
    "    c = 1\n",
    "    if len(found_vocab) > 1:\n",
    "        c = len(found_vocab)\n",
    "        \n",
    "        \n",
    "    #print('{0} word not found ,out of {1}'.format(str(len(unfound_word_glove)),str(len(v))))\n",
    "    return f/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([create_features(item) for item in range(1195)],dtype=float)\n",
    "Y = np.array([filter_data[item][1] for item in range(1195)])\n",
    "#np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X,Y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RanForestClf = RandomForestClassifier()\n",
    "RanForestClf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.5459610028\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y,RanForestClf.predict(test_x))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr = linear_model.LogisticRegression(n_jobs=1 ,C=1e5)\n",
    "Lr.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.3454038997\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y,Lr.predict(test_x))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
